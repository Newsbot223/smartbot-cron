import os
import json
import time
import hashlib
import requests
import feedparser
import re
from bs4 import BeautifulSoup
from datetime import datetime
from readability import Document

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
FEEDS = [
    "https://www.deutschlandfunk.de/nachrichten-100.rss",
    "https://www.deutschlandfunk.de/politikportal-100.rss",
    "https://www.deutschlandfunk.de/wirtschaft-106.rss",
    "https://www.spiegel.de/thema/deutschland/index.rss",
    "https://www.tagesschau.de/inland/migration-101~rss.xml",
    "https://www.spiegel.de/thema/integration/index.rss",
    "https://www.zeit.de/serie/die-aufdecker/index.xml",
    "https://www.tagesspiegel.de/feed/politik-deutschland.xml",
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://www.faz.net/rss/aktuell/politik/inland"
]

MAX_ARTICLES = 1000
MAX_TOKENS = 400
MAX_CHARS = 8000
MAX_AGE_SECONDS = 21600 # 6 —á–∞—Å–æ–≤

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
CHAT_ID = os.getenv("CHAT_ID")
BOT_TOKEN = os.getenv("BOT_TOKEN")

HEADERS = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json",
}

KEYWORDS = [
    "regierung", "bundestag", "wirtschaft", "ampel", "haushalt", "migration",
    "b√ºrgergeld", "afd", "spd", "cdu", "gr√ºne", "wahl", "streik",
    "arbeitsmarkt", "deutschland", "eu", "gesetz", "energie", "asyl", "krieg",
    "grenze", "grenzen", "grenzschutz", "bundespolizei", "fl√ºchtlinge", "einreise", "Germany",
    "German", "Berlin", "Munich", "Hamburg", "refugees", "asylum", "migrants", "integration"
]

BLOCKED_KEYWORDS = [
    "wetter", "wetterbericht", "regen", "sonnig", "heiter", "unwetter",
    "vorhersage", "temperature", "schnee", "hitze",
    "sport", "bundesliga", "fu√üball", "tor", "spiel", "trainer",
    "verein", "tabelle", "champions league", "olympia", "weltmeisterschaft",
    "spieltag", "tennis", "formel 1", "handball", "basketball"
]

STATE_DIR = "bot-state"
STATE_FILE = os.path.join(STATE_DIR, "last_file_id.json")
LOCAL_CACHE_FILE = os.path.join(STATE_DIR, "local_cache.json")
CACHE_META_FILE = os.path.join(STATE_DIR, "cache_meta.json")

os.makedirs(STATE_DIR, exist_ok=True)

def generate_filename():
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    return f"sent_articles_{timestamp}.json"

def get_cache_meta():
    """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫—ç—à–µ"""
    if os.path.exists(CACHE_META_FILE):
        try:
            with open(CACHE_META_FILE, "r") as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞: {e}")
    return {"last_update": 0, "hash": ""}

def update_cache_meta(data):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫—ç—à–µ"""
    meta = {
        "last_update": int(time.time()),
        "hash": hashlib.md5(json.dumps(data, sort_keys=True).encode("utf-8")).hexdigest(),
        "count": {
            "urls": len(data.get("urls", [])),
            "hashes": len(data.get("hashes", [])),
            "titles": len(data.get("titles", [])),
            "content_hashes": len(data.get("content_hashes", []))
        }
    }
    try:
        with open(CACHE_META_FILE, "w") as f:
            json.dump(meta, f, indent=2)
        print(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {meta}")
        return meta
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫—ç—à–∞: {e}")
        return None

def get_telegram_file_info():
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Ñ–∞–π–ª–µ –∏–∑ Telegram"""
    if not os.path.exists(STATE_FILE):
        print("‚ö† STATE_FILE –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return None
    
    try:
        with open(STATE_FILE, "r") as f:
            info = json.load(f)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å file_id
            if not info.get("file_id"):
                print("‚ö† –í STATE_FILE –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π file_id")
                return None
            return info
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ STATE_FILE: {e}")
        return None

def download_by_file_id(file_id, filename):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª –∏–∑ Telegram –ø–æ file_id"""
    if not file_id:
        print("‚ö† –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª: file_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
        return None
        
    try:
        print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ –∏–∑ Telegram —Å file_id: {file_id}")
        info = requests.get(f"https://api.telegram.org/bot{BOT_TOKEN}/getFile?file_id={file_id}").json()
        if not info.get("ok", False):
            print(f"‚ö† Telegram API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {info}")
            return None
            
        file_path = info["result"]["file_path"]
        url = f"https://api.telegram.org/file/bot{BOT_TOKEN}/{file_path}"
        data = requests.get(url).content
        with open(filename, "wb") as f:
            f.write(data)
        print(f"üì• –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω {filename} –∏–∑ Telegram –ø–æ file_id")
        return filename
    except Exception as e:
        print("‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –ø–æ file_id:", e)
        return None

def load_local_cache():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞"""
    data = {"urls": [], "content_hashes": [], "titles": [], "hashes": []}
    
    if os.path.exists(LOCAL_CACHE_FILE):
        try:
            with open(LOCAL_CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à: {LOCAL_CACHE_FILE}")
            print(f"üìä –î–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞: URLs: {len(data.get('urls', []))}, Titles: {len(data.get('titles', []))}")
            return data
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞: {e}")
    else:
        print("üìÇ –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π")
    
    return data

def load_sent_articles():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
    –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Telegram, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à.
    """
    data = {"urls": [], "content_hashes": [], "titles": [], "hashes": []}
    filename = generate_filename()
    
    print("\n" + "="*50)
    print("üîÑ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å—Ç–∞—Ç–µ–π...")
    print("="*50)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Telegram, –µ—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π file_id
    telegram_info = get_telegram_file_info()
    if telegram_info and telegram_info.get("file_id"):
        print(f"üì° –ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –≤ Telegram: {telegram_info}")
        path = download_by_file_id(telegram_info["file_id"], telegram_info.get("filename", filename))
        if path and os.path.exists(path):
            try:
                with open(path, "r", encoding="utf-8") as f:
                    loaded_data = json.load(f)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
                data["urls"] = loaded_data.get("urls", [])
                data["titles"] = loaded_data.get("titles", [])
                data["hashes"] = loaded_data.get("hashes", [])
                data["content_hashes"] = loaded_data.get("content_hashes", [])
                
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ Telegram: URLs: {len(data['urls'])}, Titles: {len(data['titles'])}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è
                save_local_cache(data)
                return data, filename
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
    else:
        print("‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ –≤ Telegram –∏–ª–∏ file_id –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Telegram, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à
    print("üìÇ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à...")
    data = load_local_cache()
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª—é—á–∏
    if "content_hashes" not in data:
        data["content_hashes"] = []
    if "urls" not in data:
        data["urls"] = []
    if "titles" not in data:
        data["titles"] = []
    if "hashes" not in data:
        data["hashes"] = []
    
    return data, filename

def save_local_cache(data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à-—Ñ–∞–π–ª"""
    try:
        with open(LOCAL_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ–±–Ω–æ–≤–ª–µ–Ω: {LOCAL_CACHE_FILE}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞
        update_cache_meta(data)
        return True
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞: {e}")
        return False

def save_sent_articles(data, local_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ñ–∞–π–ª –≤ Telegram"""
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–æ–≤
    data["urls"] = data["urls"][-MAX_ARTICLES:]
    data["hashes"] = data["hashes"][-MAX_ARTICLES:]
    data["titles"] = data.get("titles", [])[-MAX_ARTICLES:]
    data["content_hashes"] = data.get("content_hashes", [])[-MAX_ARTICLES:]

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à
    save_local_cache(data)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
    try:
        with open(local_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üìé –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏: {local_file}")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
        return False

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ Telegram
    try:
        url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendDocument"
        with open(local_file, "rb") as f:
            files = {"document": f}
            data_tg = {"chat_id": CHAT_ID, "caption": "‚úÖ –ù–æ–≤—ã–π sent_articles —Ñ–∞–π–ª"}
            res = requests.post(url, files=files, data=data_tg)

            try:
                response_json = res.json()
                print("üì¶ –û—Ç–≤–µ—Ç Telegram:", json.dumps(response_json, indent=2))
            except Exception as e:
                print("‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON-–æ—Ç–≤–µ—Ç Telegram:", e)
                response_json = {}

            file_id = response_json.get("result", {}).get("document", {}).get("file_id")
            print("üìå DEBUG: file_id =", file_id)

            if res.status_code == 200 and file_id:
                with open(STATE_FILE, "w") as meta:
                    json.dump({"file_id": file_id, "filename": local_file, "timestamp": int(time.time())}, meta)
                print(f"üì§ –û—Ç–ø—Ä–∞–≤–ª–µ–Ω {local_file}, —Å–æ—Ö—Ä–∞–Ω—ë–Ω file_id")

                os.utime(STATE_FILE, None)
                return True
            else:
                print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ñ–∞–π–ª–∞: {res.status_code}")
                return False
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ñ–∞–π–ª–∞ –≤ Telegram: {e}")
        return False

def get_article_text(url):
    try:
        response = requests.get(url, timeout=10)
        html = response.text
        doc = Document(html)
        summary = doc.summary()
        text = BeautifulSoup(summary, "html.parser").get_text(separator="\n", strip=True)
        return text
    except Exception as e:
        print("‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞—Ç—å–∏:", e)
        return ""

def normalize_text(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not text:
        return ""
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = re.sub(r'\s+', ' ', text.lower()).strip()
    # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    text = re.sub(r'[^\w\s]', '', text)
    return text

def clean_article_text(text):
    """–£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
    text = re.sub(r'^Titel:\s*', '', text, flags=re.IGNORECASE | re.MULTILINE)
    text = re.sub(r'Diese Nachricht wurde am .*? gesendet\.', '', text, flags=re.IGNORECASE | re.DOTALL)
    return text.strip()

def get_content_hash(text):
    """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à —Ç–æ–ª—å–∫–æ –æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏"""
    # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Ö–µ—à–∞
    normalized_text = normalize_text(text[:1000])
    return hashlib.md5(normalized_text.encode("utf-8")).hexdigest()

def is_duplicate_content(text, sent_data):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç–∞—Ç—å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é"""
    if not text:
        return False
    
    # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –æ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    content_hash = get_content_hash(text)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–∞–∫–æ–π —Ö–µ—à –≤ –∏—Å—Ç–æ—Ä–∏–∏
    if content_hash in sent_data.get("content_hashes", []):
        print("üîÑ –î—É–±–ª–∏–∫–∞—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ —Ö–µ—à—É —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è")
        return True
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è —Å–ª—É—á–∞–µ–≤ –Ω–µ–±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    normalized_text = normalize_text(text[:500])
    for i, old_hash in enumerate(sent_data.get("content_hashes", [])):
        # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã —Å—Ä–∞–≤–Ω–∏—Ç—å –Ω–∞–ø—Ä—è–º—É—é
        # –ù–æ —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å —Ç–æ–ª—å–∫–æ —Ö–µ—à–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ —Å—Ç–∞—Ä–æ–º—É –∞–ª–≥–æ—Ä–∏—Ç–º—É
        if i < len(sent_data.get("hashes", [])):
            old_hash_base = sent_data.get("hashes", [])[i]
            if old_hash_base and normalized_text and old_hash_base.startswith(normalized_text[:20]):
                print("üîÑ –î—É–±–ª–∏–∫–∞—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é")
                return True
    
    return False

def summarize(text):
    prompt = f'''
Lies bitte den folgenden Text sorgf√§ltig (auch wenn er auf Englisch ist) und fasse ihn dann auf DEUTSCH zusammen.

Fasse den Inhalt in 4‚Äì7 S√§tzen auf DEUTSCH zusammen.
‚û§ Verwende kurze Abs√§tze.
‚û§ Schreibe stilistisch ansprechend im Stil einer Nachrichtenseite (z.‚ÄØB. tagesschau.de).
‚û§ Kein Kommentar, keine Meinung, keine √úbersetzung ‚Äì nur eine sachliche, journalistische Zusammenfassung auf Deutsch.

‚ö†Ô∏è Wichtig: Der ganze Output soll ausschlie√ülich auf DEUTSCH sein ‚Äì auch wenn der Ursprungstext auf Englisch oder einer anderen Sprache ist.

Text: {text}
'''
    try:
        res = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=HEADERS, json={
            "model": "mistralai/mistral-7b-instruct",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.7,
            "max_tokens": MAX_TOKENS
        }, timeout=60)
        res.raise_for_status()
        result = res.json()
        if "choices" in result and isinstance(result["choices"], list):
            return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print("Fehler bei Zusammenfassung:", e)
        return ""

def send_message(text):
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": False
    }
    return requests.post(url, json=payload).status_code == 200

def main():
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
    print("\n" + "="*50)
    print("üöÄ –ó–ê–ü–£–°–ö –ë–û–¢–ê –î–õ–Ø –ù–û–í–û–°–¢–ï–ô")
    print("="*50)
    
    if os.path.exists(LOCAL_CACHE_FILE):
        cache_mtime = datetime.fromtimestamp(os.path.getmtime(LOCAL_CACHE_FILE))
        print(f"üìÇ –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {cache_mtime}")
    else:
        print("üìÇ –õ–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
        
    if os.path.exists(STATE_FILE):
        state_mtime = datetime.fromtimestamp(os.path.getmtime(STATE_FILE))
        print(f"üìÇ STATE_FILE —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {state_mtime}")
        
        # –í—ã–≤–æ–¥–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ STATE_FILE –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        try:
            with open(STATE_FILE, "r") as f:
                state_content = json.load(f)
            print(f"üìÑ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ STATE_FILE: {state_content}")
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ STATE_FILE: {e}")
    else:
        print("üìÇ STATE_FILE –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
    sent, local_file = load_sent_articles()
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ URLs: {len(sent.get('urls', []))}, Titles: {len(sent.get('titles', []))}, Hashes: {len(sent.get('hashes', []))}, Content Hashes: {len(sent.get('content_hashes', []))}")
    
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–ª—é—á–∏
    if "content_hashes" not in sent:
        sent["content_hashes"] = []

    for feed_url in FEEDS:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            url = entry.link
            title = entry.title

            # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ URL –∏ –∑–∞–≥–æ–ª–æ–≤–∫—É
            if url in sent["urls"] or title in sent["titles"]:
                print(f"‚è© Bereits verarbeitet: {title}")
                continue

            published = entry.get("published_parsed")
            if published:
                pub_date = datetime.fromtimestamp(time.mktime(published))
                if (datetime.utcnow() - pub_date).total_seconds() > MAX_AGE_SECONDS:
                    print(f"‚è≥ Zu alt, √ºbersprungen: {title}")
                    continue

            full_text = get_article_text(url)
            full_text = clean_article_text(full_text)

            if len(full_text) > MAX_CHARS:
                print(f"‚ö† Zu lang, √ºbersprungen: {title} ({len(full_text)} Zeichen)")
                continue

            if len(full_text) < 200:
                print(f"‚ö† √úbersprungen ({feed_url}): {title} (zu kurz)")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if not any(keyword.lower() in full_text.lower() for keyword in KEYWORDS):
                print(f"‚õî Thema nicht relevant: {title}")
                continue

            if any(word in full_text.lower() for word in BLOCKED_KEYWORDS):
                print(f"‚ùå Thema blockiert: {title}")
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            if is_duplicate_content(full_text, sent):
                print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {title}")
                continue

            # –°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            hash_base = (title + full_text[:300].lower()).strip()
            hash_ = hashlib.md5(hash_base.encode("utf-8")).hexdigest()
            if hash_ in sent["hashes"]:
                print(f"üîÑ –î—É–±–ª–∏–∫–∞—Ç –ø–æ —Å—Ç–∞—Ä–æ–º—É —Ö–µ—à—É: {title}")
                continue

            # –ù–æ–≤—ã–π –º–µ—Ç–æ–¥ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_hash = get_content_hash(full_text)

            print(f"üîÑ Analysiere: {title}")
            summary = summarize(full_text)
            if not summary:
                continue

            caption = f"<b>üì∞ {title}</b>\n\n{summary}\n\nüîó <a href='{url}'>Weiterlesen</a>"

            success = send_message(caption)
            if success:
                print("‚úÖ Gesendet")
                sent["urls"].append(url)
                sent["hashes"].append(hash_)
                sent["content_hashes"].append(content_hash)
                if title not in sent["titles"]:
                    sent["titles"].append(title)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫—ç—à –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏
                save_local_cache(sent)
            else:
                print("‚ö† Fehler beim Senden")

    save_sent_articles(sent, local_file)
    print("\n" + "="*50)
    print("üèÅ –ó–ê–í–ï–†–®–ï–ù–ò–ï –†–ê–ë–û–¢–´ –ë–û–¢–ê")
    print("="*50)

if __name__ == "__main__":
    main()
